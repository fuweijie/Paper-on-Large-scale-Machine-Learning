# Paper-on-Large-scale-Machine-Learning
### The following are some suggeestions for papers on large scale machine learning.
### Surveys
O. Y. Al-Jarrah, P. D. Yoo, S. Muhaidat, G. K. Karagiannidis, and K. Taha, “Efficient machine learning for big data: A review,” Big Data Research, vol. 2, no. 3, pp. 87–93, 2015.

L. Bottou, F. E. Curtis, and J. Nocedal, “Optimization methods for large-scale machine learning,” SIAM Review, vol. 60, no. 2, pp. 223–311, 2018.

C.-W. Tsai, C.-F. Lai, H.-C. Chao, and A. V. Vasilakos, “Big data analytics: a survey,” Journal of Big data, vol. 2, no. 1, p. 21, 2015.

S. Landset, T. M. Khoshgoftaar, A. N. Richter, and T. Hasanin, “A survey of open source tools for machine learning with big data in the hadoop ecosystem,” Journal of Big Data, vol. 2, no. 1, p. 24, 2015.

### We present a comprehensive overview of LML according to three computational perspectives:
1) model simplification, which reduces computational complexities by simplifying predictive models; 
2) optimization approximation, which enhances computational efficiency by designing better optimization algorithms; 
3) computation parallelism, which improves computational capabilities by scheduling multiple computing devices.
## Model Simplification.
### Kernel-based Models
S. Kumar, M. Mohri, and A. Talwalkar, “Sampling methods for the nystrom method,” JMLR, vol. 13, no. Apr, pp. 981–1006, 2012.
### Graph-based Models
### Deep Models
### Tree-based Models
## Optimization Approximation.
## Computation Parallelism.
## Hybrid Collaboration.
